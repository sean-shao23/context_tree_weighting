# TBD
## update with README + links to report + slides

  This repository contains a full funcitonal implementation of the Context Tree Weighting compression algorithm, coded in Python 3, which presents an performance improvement to a Markov (Adaptive Arithmetic) Model by mixing the predictions of many Markov models of differing lengths in order to improve the adaptability of the predictor. This results in good performance for all source sequence lengths, not just infinitely long sequences. We implemented CTW in the Stanford Compression Library (SCL), which can be found at the following link: https://github.com/kedartatwawadi/stanford_compression_library. Because the goal of SCL is “not meant to be fast or efficient implementation, but rather for educational purpose”, our implementation is focused on being as straightforward to understand as possible. As a result, our code is not as optimized as other implementations, and only works on binary alphabets, but it still serves its purpose of both demonstrating how the context tree weighting method works and how its performance compares to other compression techniques. 

  In our CTW implementation, we use a weighted context tree, which is where each node corresponds to a context, and each context is assigned a probability based on an input sequence. Based on the context probabilities, we assign each node a probability which is a weighted mixture of its own context’s probability and its children’s node probabilities. The node probability of the root of the context tree represents the probability of the input sequence. We use these probabilities to estimate the symbol probabilities. In our implementation, we only support binary alphabets since our context tree is a binary tree, but it is possible to use larger context trees to support larger alphabets. Additional information on the theory and technical details of the CTW algorithm can be found in our Final Report at the link provided below. 

  The three core classes of our implementation are CTWNode, CTWTree, and CTWModel, the latter being at the core of the CTW method, aiming to model the behavior of the source of the input. The three classes allow the algorithm to process an input source through the encoding and decoding steps, while updating the binary tree at every step through the provided context. Further information on these classes can be found in the Final Report, attached below. 

  As expected, performance evaluations on our algorithm show a linear relationship between the conpression time and both the length of the input source and the tree size, and an exponential relationship between the memory footprint and the depth of the tree. Results also show little discrepancy in performance for the CTW compressor compared to a kth-order Adaptive Arithmetic Encoder for an exact order Markov source, while instead a notable speedup for the CTW compressor compared to a kth-order Adaptive Arithmetic Encoder for a lower order tree source. Additional results yielded by our performance evaluations can be found in the Final Report at the provided link below.  

  Future work for this project includes implementing branch pruning, which would reduce the static memory usage of our model, going from constant to the input size but exponential to the tree height, to linear in both input size and tree height. Additionally, since our algorithm is only intended to process binary alphabets, an additioal extension to this project could be implementing Volf’s decomposed CTW algorithm (https://www.sps.tue.nl/wp-content/uploads/2015/09/Volf2002PhDthesis.pdf), which extends the CTW algorithm to handle larger alphabets, such as using a 256-ary context tree for encoding one Unicode character at a time. Finally, another possible extension could be replacing the Krichevsky-Trofimov (KT) estimator with an adaptive or windowed KT estimator to deal with non-stationary or piecewise sequences, respectively. Alternatively, the KT estimator could be replaced with an entirely different iid estimator.


Link to Final Report: https://docs.google.com/document/d/1us2u3a0XirPm2olOTUo2SawydYO8NIY298SC8sohmnE/edit?usp=sharing
Link to Final Presentation: https://docs.google.com/presentation/d/1OBl4hdcz0h3uKht-4pY9Dg5KSpwTNX_O-P-hVNoShI8/edit?usp=sharing


